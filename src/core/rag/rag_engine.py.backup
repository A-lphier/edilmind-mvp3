"""
src/core/rag/rag_engine.py
RAG Engine con ChromaDB v0.4+ (API AGGIORNATA)
"""
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

from sentence_transformers import SentenceTransformer
import chromadb

logger = logging.getLogger(__name__)

class RAGEngine:
    """Motore RAG con ChromaDB v0.4+"""
    
    def __init__(
        self,
        vector_store_path: str = "data/vector_store",
        model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    ):
        self.vector_store_path = Path(vector_store_path)
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üîÑ Caricamento modello: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        # ‚úÖ NUOVA API
        self.client = chromadb.PersistentClient(path=str(self.vector_store_path))
        
        self.collections = {
            "normative": self._get_or_create_collection("normative"),
            "circolari": self._get_or_create_collection("circolari"),
            "giurisprudenza": self._get_or_create_collection("giurisprudenza"),
            "tecniche": self._get_or_create_collection("tecniche")
        }
        
        logger.info("‚úÖ RAG Engine inizializzato")
    
    def _get_or_create_collection(self, name: str):
        try:
            return self.client.get_collection(name=name)
        except:
            return self.client.create_collection(name=name, metadata={"hnsw:space": "cosine"})
    
    def add_document(self, text: str, category: str, metadata: Dict, doc_id: Optional[str] = None) -> str:
        if category not in self.collections:
            raise ValueError(f"Categoria non valida: {category}")
        
        collection = self.collections[category]
        chunks = self._intelligent_chunk(text)
        
        if not chunks:
            logger.warning("Nessun chunk creato")
            return None
        
        embeddings = self.model.encode(chunks, show_progress_bar=False)
        
        if doc_id is None:
            doc_id = f"{category}_{datetime.now().timestamp()}"
        
        ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
        metadatas = [{**metadata, "chunk_id": i, "doc_id": doc_id} for i in range(len(chunks))]
        
        collection.upsert(ids=ids, embeddings=embeddings.tolist(), documents=chunks, metadatas=metadatas)
        
        logger.info(f"‚úÖ Documento {doc_id}: {len(chunks)} chunks")
        return doc_id
    
    def _intelligent_chunk(self, text: str, max_len: int = 1000) -> List[str]:
        import re
        chunks = []
        articles = re.split(r'\n\s*(?:Art\.|Articolo)\s*\d+', text)
        
        if len(articles) > 1:
            for article in articles[1:]:
                if len(article) > max_len:
                    paras = article.split('\n\n')
                    current = ""
                    for para in paras:
                        if len(current) + len(para) < max_len:
                            current += "\n\n" + para
                        else:
                            if current:
                                chunks.append(current.strip())
                            current = para
                    if current:
                        chunks.append(current.strip())
                else:
                    chunks.append(article.strip())
        else:
            paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 100]
            current_chunk = ""
            for para in paragraphs:
                if len(current_chunk) + len(para) < max_len:
                    current_chunk += "\n\n" + para
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        return [c for c in chunks if len(c) > 50]
    
    def query(self, query_text: str, categories: List[str] = None, top_k: int = 5, filters: Dict = None) -> List[Dict]:
        if categories is None:
            categories = list(self.collections.keys())
        
        query_embedding = self.model.encode([query_text])[0]
        results = []
        
        for cat in categories:
            if cat not in self.collections:
                continue
            
            collection = self.collections[cat]
            
            try:
                res = collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_k, where=filters)
                
                if res['ids'] and res['ids'][0]:
                    for i in range(len(res['ids'][0])):
                        results.append({
                            'category': cat,
                            'text': res['documents'][0][i],
                            'metadata': res['metadatas'][0][i],
                            'distance': res['distances'][0][i] if 'distances' in res else None,
                            'relevance_score': 1 - res['distances'][0][i] if 'distances' in res else None
                        })
            except Exception as e:
                logger.error(f"Errore query {cat}: {e}")
        
        results.sort(key=lambda x: x['relevance_score'] or 0, reverse=True)
        return results[:top_k]
    
    def build_context(self, query_results: List[Dict], max_tokens: int = 3000) -> str:
        if not query_results:
            return ""
        
        context = "### CONTESTO NORMATIVO RILEVANTE ###\n\n"
        current_len = len(context)
        max_chars = max_tokens * 4
        
        for i, result in enumerate(query_results, 1):
            metadata = result['metadata']
            text = result['text']
            
            source_header = f"**Fonte {i}:** {metadata.get('titolo', 'N/D')}"
            if metadata.get('document_type'):
                source_header += f" ({metadata['document_type']})"
            if metadata.get('anno'):
                source_header += f" - Anno {metadata['anno']}"
            
            source_header += f"\n*Categoria: {result['category']}*\n"
            source_header += f"*Rilevanza: {result['relevance_score']:.2%}*\n\n"
            
            chunk_text = f"{source_header}{text}\n\n---\n\n"
            
            if current_len + len(chunk_text) > max_chars:
                break
            
            context += chunk_text
            current_len += len(chunk_text)
        
        context += "### FINE CONTESTO ###\n\nUsa le informazioni sopra per rispondere. Cita le fonti.\n"
        return context
    
    def get_statistics(self) -> Dict:
        stats = {}
        for name, collection in self.collections.items():
            try:
                stats[name] = collection.count()
            except:
                stats[name] = 0
        stats['total'] = sum(stats.values())
        return stats

def load_knowledge_base_from_folder(kb_root: str = "data/knowledge_base"):
    from tqdm import tqdm
    import PyMuPDF as fitz
    from docx import Document as DocxDocument
    
    rag = RAGEngine()
    kb_path = Path(kb_root)
    
    files = []
    for ext in ['*.pdf', '*.docx', '*.txt', '*.md']:
        files.extend(kb_path.rglob(ext))
    
    logger.info(f"üìö Trovati {len(files)} documenti")
    processed = 0
    
    for file_path in tqdm(files, desc="Processing KB"):
        try:
            category = "normative"
            if "circolar" in str(file_path).lower():
                category = "circolari"
            elif "giurisprudenza" in str(file_path).lower():
                category = "giurisprudenza"
            elif "tecnic" in str(file_path).lower():
                category = "tecniche"
            
            if file_path.suffix == '.pdf':
                doc = fitz.open(file_path)
                text = "\n".join([page.get_text() for page in doc])
                doc.close()
            elif file_path.suffix == '.docx':
                doc = DocxDocument(file_path)
                text = "\n".join([p.text for p in doc.paragraphs])
            else:
                text = file_path.read_text(encoding='utf-8')
            
            metadata = {
                'filename': file_path.name,
                'titolo': file_path.stem.replace('_', ' ').title(),
                'upload_date': datetime.now().isoformat()
            }
            
            result = rag.add_document(text, category, metadata)
            if result:
                processed += 1
        except Exception as e:
            logger.error(f"Errore {file_path.name}: {e}")
    
    logger.info(f"‚úÖ Processati {processed}/{len(files)}")
    stats = rag.get_statistics()
    logger.info(f"üìä Vector Store: {stats}")
    return stats

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    print("üß™ Test RAG Engine...")
    stats = load_knowledge_base_from_folder()
    rag = RAGEngine()
    results = rag.query("requisiti SOA", top_k=3)
    print(f"\nüîç Query results:")
    for r in results:
        print(f"  - {r['metadata'].get('titolo', 'N/D')} ({r['relevance_score']:.2%})")